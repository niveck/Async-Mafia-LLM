{
    "model_name": "meta-llama/Llama-3.1-8B-Instruct",
    "use_pipeline": false,
    "pipeline_task": "text-generation",
    "max_new_tokens": 25,
    "num_beams": 1,
    "repetition_penalty": 1.25,
    "do_sample": true,
    "temperature": 1.3,
    "no_repeat_ngram_size": 8,
    "num_words_per_second_to_wait": 3,
    "pass_turn_token": "<wait>",
    "use_turn_token": "<send>",
    "async_type": "every_x_messages"
}